{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters for the experiment. num_data is how many records are generated, and num_minutes is how many minutes the system will run requests.\n",
    "num_data = 100000\n",
    "num_minutes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printf feast_speed_testing/**\\\\ndata/**\\\\n.*\\\\n*.ipynb\\\\nregistry.db > .feastignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'feast[postgres, redis]' psycopg2 kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data and move it to a suitable (non-hidden) location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "# https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
    "path = kagglehub.dataset_download(\"rashikrahmanpritom/heart-attack-analysis-prediction-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /opt/app-root/src/.cache/kagglehub/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/versions/2 /opt/app-root/src/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import arange\n",
    "\n",
    "data = read_csv(\"../data/heart.csv\")\n",
    "# TODO: Drop extra feature columns here.\n",
    "\n",
    "# We add an ID column to demonstrate repeatability.\n",
    "data['patient_id'] = arange(len(data))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from scipy.stats import describe, truncnorm\n",
    "from numpy import sqrt, round, int64\n",
    "from numpy.random import SeedSequence\n",
    "\n",
    "def generate_data(source_data: DataFrame, target_column: str = \"output\", num_samples: int = 100):\n",
    "    generated_data = DataFrame()\n",
    "    sequence = SeedSequence(123)\n",
    "    for column in source_data.columns:\n",
    "        if column != target_column:\n",
    "            next_seed = sequence.spawn(1)[0]\n",
    "            column_stats = describe(source_data[column])\n",
    "            a, b = (column_stats.minmax[0] - column_stats.mean) / sqrt(column_stats.variance), (column_stats.minmax[1] - column_stats.mean) / sqrt(column_stats.variance)\n",
    "            column_data = truncnorm.rvs(a=a, b=b, loc=column_stats.mean, scale=sqrt(column_stats.variance), size=num_samples, random_state=next_seed.generate_state(1)[0])\n",
    "\n",
    "            column_type = source_data[column].dtype\n",
    "            if column_type == int64:\n",
    "                column_data = round(column_data)\n",
    "            column_data = column_data.astype(column_type)\n",
    "\n",
    "            generated_data[column] = column_data\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from numpy import arange\n",
    "\n",
    "timestamps = [datetime.now() for i in range(num_data)]\n",
    "\n",
    "generated_data = generate_data(data, num_samples = num_data)\n",
    "generated_data[\"patient_id\"] = arange(num_data)\n",
    "generated_data[\"event_timestamp\"] = timestamps\n",
    "generated_data[\"created\"] = timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the generated data off to the offline feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from sqlalchemy import create_engine, engine\n",
    "\n",
    "connection_string = engine.URL.create(\n",
    "    drivername=\"postgresql\",\n",
    "    username=getenv('DB_USERNAME'),\n",
    "    password=getenv('DB_PASSWORD'),\n",
    "    host=getenv('DB_HOST'),\n",
    "    database=getenv('DB_NAME'),\n",
    ")\n",
    "\n",
    "this_engine = create_engine(connection_string)\n",
    "generated_data.to_sql('heart_values', schema=\"feast\", con=this_engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import connect\n",
    "\n",
    "try:\n",
    "    conn = connect(dbname=getenv('DB_NAME'), user=getenv('DB_USERNAME'), host=getenv('DB_HOST'), password=getenv('DB_PASSWORD'))\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "\n",
    "with conn.cursor() as curs:\n",
    "    curs.execute(\"SELECT count(1) from feast.heart_values where patient_id < 1000\")\n",
    "    single_row = curs.fetchone()\n",
    "    print(single_row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialize the data into the online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!feast materialize 2024-12-01T00:00:00 2026-01-01T00:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(151123)\n",
    "\n",
    "y = data['output']\n",
    "X = data.drop(columns=['output'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "# Fun fact, if you don't drop the ID column, you get perfect accuracy.\n",
    "X_train = X_train.drop(columns=[\"patient_id\"])\n",
    "X_test = X_test.drop(columns=[\"patient_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that heart_repo.py's data in transformed_data() matches the values you get here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_rescaled_train = scaler.fit_transform(X_train)\n",
    "X_rescaled_test = scaler.transform(X_test)\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is trained here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_rescaled_train, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(X_rescaled_test)\n",
    "\"\"\"\n",
    "The confusion matrix when including all the features should match this:\n",
    "array([[33,  9],\n",
    "       [10, 48]])\n",
    "\"\"\"\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the feature store to prepare for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart_repo import heart_v1\n",
    "from numpy import random, array\n",
    "from pandas import DataFrame\n",
    "\n",
    "def run():\n",
    "    patient_id = random.randint(0, num_data, 1)[0]\n",
    "\n",
    "    features = store.get_online_features(\n",
    "        features=heart_v1,\n",
    "        entity_rows=[\n",
    "            {\n",
    "                \"patient_id\": patient_id\n",
    "            }\n",
    "        ],\n",
    "    ).to_df()\n",
    "    return model.predict(features.drop(columns=[\"patient_id\"]).to_numpy())\n",
    "\n",
    "# Because of how Python decorators work, you can't just import the function. This function matches heart_repo.transformed_data exactly, however.\n",
    "# TODO: Drop feature columns here, and remove the corresponding records from the 2 arrays in the `features` definition.\n",
    "def run_in_memory(this_row: DataFrame):\n",
    "    raw_features = this_row[['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
    "       'exng', 'oldpeak', 'slp', 'caa', 'thall']]\n",
    "    features = (raw_features - array([5.46502463e+01, 6.60098522e-01, 1.00985222e+00, 1.30812808e+02,\n",
    "        2.48448276e+02, 1.28078818e-01, 5.27093596e-01, 1.49655172e+02,\n",
    "        3.30049261e-01, 1.03300493e+00, 1.40394089e+00, 6.35467980e-01,\n",
    "        2.33004926e+00])) / array([ 8.99758246,  0.47367548,  1.04098356, 17.02951461, 54.87667351,\n",
    "         0.33417755,  0.52803654, 22.61651409,  0.47023052,  1.09919108,\n",
    "         0.61538905,  0.91787029,  0.59920596])\n",
    "    return model.predict([features.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the number of minutes elapsed so Jupyter doesn't timeout its connection.\n",
    "from time import time\n",
    "from numpy.random import seed\n",
    "seed(12351)\n",
    "\n",
    "start_time = time()\n",
    "end_time = start_time + 60 * num_minutes\n",
    "printed_time = False\n",
    "counts = 0\n",
    "\n",
    "while time() < end_time:\n",
    "    time_elapsed = int(time() - start_time)\n",
    "    if time_elapsed % 60 == 0:\n",
    "        if not printed_time:\n",
    "            print(f\"{time_elapsed / 60} minutes elapsed\")\n",
    "            printed_time = True\n",
    "    else:\n",
    "        printed_time = False\n",
    "    run()\n",
    "    counts += 1\n",
    "counts\n",
    "\n",
    "seed(12351)\n",
    "\n",
    "start_time = time()\n",
    "end_time = start_time + 60 * num_minutes\n",
    "printed_time = False\n",
    "counts_in_memory = 0\n",
    "\n",
    "while time() < end_time:\n",
    "    time_elapsed = int(time() - start_time)\n",
    "    if time_elapsed % 60 == 0:\n",
    "        if not printed_time:\n",
    "            print(f\"{time_elapsed / 60} minutes elapsed\")\n",
    "            printed_time = True\n",
    "    else:\n",
    "        printed_time = False\n",
    "    run_in_memory(generated_data.iloc[random.randint(0, num_data, 1)[0]])\n",
    "    counts_in_memory += 1\n",
    "counts_in_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This are the final iteration counts when run for the number of minutes provided\n",
    "\n",
    "This is the literal definition of a Poisson Distribution (I.I.D. events in a fixed interval of time and we want the count), so variance = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "\n",
    "print(f\"The number of calls in {num_minutes} minutes is:\")\n",
    "print(f\"{counts} +- {sqrt(counts)} using Feast\")\n",
    "print(f\"{counts_in_memory} +- {sqrt(counts_in_memory)}  calculating on the fly entirely in memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of interpolated calls in 24 hours is:\")\n",
    "print(f\"{counts * 24 * 60 / num_minutes} +- {sqrt(counts * 24 * 60 / num_minutes)} using Feast\")\n",
    "print(f\"{counts_in_memory * 24 * 60 / num_minutes} +- {sqrt(counts_in_memory * 24 * 60 / num_minutes)}  calculating on the fly entirely in memory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
